{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the demo experiments\n",
    "\n",
    "This notebook demonstrates how the experiment results can be loaded and analyzed.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You need to a week of Puffer data and run the demo experiments first:\n",
    "\n",
    "```bash\n",
    "./demo.py download\n",
    "./demo.py run-demo\n",
    "```\n",
    "\n",
    "The demo contains and evaluation and a replay. The evaluation is rather quick\n",
    "(less than an hour, depending on the machine), but the replay can take several\n",
    "hours due to the required model training.\n",
    "\n",
    "If you only want to run only the evaluation or analysis, use the commands:\n",
    "\n",
    "```bash\n",
    "./demo.py run-demo analysis\n",
    "./demo.py run-demo replay\n",
    "```\n",
    "\n",
    "See `REAMDE.md` for additional CLI options such as verbosity, running experiments in parallel, or scheduling jobs with Slurm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # To allow imports from parent directory\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import experiment_helpers as eh\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot import and configuration.\n",
    "%matplotlib inline\n",
    "\n",
    "# Set theme and parameters\n",
    "eh.plot_utils.setup(latex=False)\n",
    "\n",
    "# Load the config, including the local config.\n",
    "from config import Config as LocalConfig  # Load from file: ../config.py\n",
    "from experiments.puffer.config import PufferExperimentConfig\n",
    "config = PufferExperimentConfig.with_updates(LocalConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puffer Data Analysis\n",
    "\n",
    "In the following, we show the evaluation results obtained from running the demo.\n",
    "\n",
    "For more in-depth visualization and to recreate the paper plots, go to the [puffer-analysis notebook](./puffer-analysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation results.\n",
    "experiment_directory = config.output_directory / \"run-demo/analysis\"\n",
    "\n",
    "_frames = []\n",
    "for daydir in experiment_directory.iterdir():\n",
    "    _frames.append(eh.data.read_csv(daydir / \"results.csv.gz\"))\n",
    "results = pd.concat(_frames, ignore_index=True)\n",
    "\n",
    "# Have a peek\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in [long (also called narrow) format][1]. The context of each value is given by:\n",
    "- `day`: The evaluated day.\n",
    "- `abr` The evaluated algorithm.\n",
    "- `selection`: We evaluate three subsets of data.\n",
    "  - `all`: All data.\n",
    "  - `stalled`: Only results of sessions experiencing stalls.\n",
    "  - `unstalled`: Only results of sessions not experiencing stalls.\n",
    "- `variable`: Which aspect of the data is evaluated. Some important variables are:\n",
    "  - `ssim`: Image quality of all video chunks.\n",
    "  - `stream`: Time info for the stream, e.g. playtime and rebuffering time.\n",
    "- `metric`: For each variable, we compute several aggregate statistics. E.g. for SSIM, we evaluate mean, std, and several percentiles.\n",
    "\n",
    "As an example, let's plot the average SSIM, fraction of time spent stalled, and prediction scores per algorithm and day, for all sessions, for Fugu (static, labels fugu-feb) and the default version of Memento.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Wide_and_narrow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = (\n",
    "    results\n",
    "    # Select data.\n",
    "    .query(\"(selection == 'all') and (abr in ('fugu-feb', 'memento'))\")\n",
    "    .drop(columns=[\"selection\"])\n",
    "    .query(\n",
    "        \"(variable == 'ssim' and metric == 'mean') or \"\n",
    "        \"(variable == 'stream' and metric == 'stalled') or \"\n",
    "        \"(variable == 'logscore' and metric == 'mean') or \"\n",
    "        \"(variable == 'logscore' and metric == '0.01')\"\n",
    "    )\n",
    "    # Reshape and round values for nicer visualization\n",
    "    .pivot(\n",
    "        index=[\"day\"],\n",
    "        columns=[\"variable\", \"metric\", \"abr\"],\n",
    "        values=\"value\",\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "display(aggregated)\n",
    "display(aggregated.mean().to_frame(\"Mean\").T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an almost on-par image quality (on average 0.5% worse), Memento spends a smaller fraction of streamtime stalled than Fugu (on average 18% less).\n",
    "The prediction score is slightly worse for Memento on average and significantly better at the tail.\n",
    "\n",
    "Both ABRs use the same algorithm and ML model architecture, but Memento selects training samples to cover the whole sample space.\n",
    "As a result, we are more robust to rare cases, which shows in the reduction of stalls. We pay a small price in image quality, as we learn to predict the most common cases a little worse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puffer Data Replay\n",
    "\n",
    "In this experiment, we replay the data from the demo week, to train a Fugu model from scratch using samples selected by Memento.\n",
    "\n",
    "For each day of the week, we feed the data into Memento, which updates a training data set (which contains at most a million samples).\n",
    "\n",
    "This model is evaluated on the data of the next day.\n",
    "The Fugu (static) model is also evaluated on this data as a comparison point.\n",
    "\n",
    "Note that during this experiment, we train and evaluate every day to get a close look at the samples selected at every step. In deployment, Memento would not retrain if the sample set does not change significantly.\n",
    "\n",
    "We replay data to analyze the impact of various parameters on the quality of Memento. For a more in-depth analysis and to recreate the figures from the paper, see the [puffer-replay notebook](./puffer-replay.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_directory = config.output_directory / \"run-demo/replay\"\n",
    "replay_results = eh.data.read_csv(replay_directory / \"results.csv.gz\")\n",
    "\n",
    "# Have a look at the raw data.\n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results contain a range of aggregation metrics (in the `metric` column) for different variables (in separate columns).\n",
    "\n",
    "As an example, we plot the mean and tail prediction score over iterations for both the Memento-trained model and the Fugu (static) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_results = (\n",
    "    replay_results\n",
    "    .query(\"metric in ('mean', '0.01')\")\n",
    "    .melt(id_vars=[\"iteration\", \"metric\"],\n",
    "          value_vars=[\"logscore\", \"fugu_feb_logscore\"])\n",
    ")\n",
    "selected_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=selected_results,\n",
    "    kind=\"line\",\n",
    "    x=\"iteration\", \n",
    "    y=\"value\",\n",
    "    style=\"metric\",\n",
    "    hue=\"variable\",\n",
    ").set(\n",
    "    xlabel=\"Days replayed\",\n",
    "    ylabel=\"Logscore\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean prediction scores are almost equal, with Fugu (static) being slightly better.\n",
    "\n",
    "At the tail, however, Memento is significantly better, as it has seen more rare cases during training, even after after only a couple of days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at the `stats.csv.gz`, which contains additional info about the retraining decisions process.\n",
    "\n",
    "In the demo, we retrain every day to see the effects of sample selection.\n",
    "In deployment, we only retrain if the estimated coverage of sample space increases by 10%. Let's plot this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = eh.data.read_csv(replay_directory / \"stats.csv.gz\")\n",
    "display(stats.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=stats,\n",
    "    kind=\"line\",\n",
    "    x=\"iteration\",\n",
    "    y=\"coverage_increase\"\n",
    ").set(\n",
    "    xlabel=\"Days replayed\",\n",
    "    ylabel=\"Relative coverage increase\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that day 0 is excluded. Without previous samples, we cannot estimate a difference in coverage and always retrain.\n",
    "\n",
    "We can see that the coverage increase is above 0.1 only for day 1, so Memento would have retrained on days 0 and 1 and stopped after, as newly observed samples are already covered by the sample selection and retraining would be unlikely to yield improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11-memento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
